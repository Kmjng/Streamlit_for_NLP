# -*- coding: utf-8 -*-
"""
Created on Fri Sep 20 11:12:56 2024

@author: user

# prompt ë¡œ ì‹¤í–‰ 
cd C:/ITWILL/Streamlit_for_NLP
streamlit run app.py
"""
import numpy as np
import streamlit as st
 
from collections import Counter
import re
import matplotlib.pyplot as plt

from wordcloud import WordCloud

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import pandas as pd 
from textblob import TextBlob # ê°ì„±ë¶„ì„ì„ ìœ„í•œ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ 

import gensim
from gensim import corpora # ë‹¨ì–´ í† í°ê³¼ ë§¤ì¹­

import pyLDAvis
import pyLDAvis.gensim_models as gensimvis

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('punkt_tab') # NLTK punkt ë¦¬ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ (í† í°í™”ì— ì“°ìž„)

# ì˜ë¬¸ ë¶ˆìš©ì–´ ì²˜ë¦¬

# NLTK ë¶ˆìš©ì–´ ë‹¤ìš´ë¡œë“œ
nltk.download('stopwords')

# ì‚¬ìš©ìž í•¨ìˆ˜ ì •ì˜ 
def extract_words(text):
    # ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë§Œ ì¶”ì¶œí•˜ê³  ì†Œë¬¸ìžë¡œ ë³€í™˜
    words = re.findall(r'\b\w+\b', text.lower())
    stop_words = set(stopwords.words('english'))
    # ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    words = [word for word in words if word not in stop_words]
    # ì–´ê°„ì¶”ì¶œ (í‘œì œì–´ ì¶”ì¶œ, Lemmatization í™œìš©)
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word, pos = 'v') for word in words] # ë™ì‚¬ í‘œì œì–´ ì¶”ì¶œ
    lemmatized_words = [lemmatizer.lemmatize(word, pos = 'n') for word in lemmatized_words] # ëª…ì‚¬ í‘œì œì–´ ì¶”ì¶œ
    return lemmatized_words

#################
######### ê¸°ëŠ¥ 1. 
#################
st.title("1. Word Count - ì˜ë¬¸í…ìŠ¤íŠ¸ version")
st.subheader("ë“±ìž¥í•œ ë‹¨ì–´ë“¤ì„ ì„¸ì–´ ì¤ë‹ˆë‹¤")

# ì‚¬ìš© ë°©ë²• í”Œë¡œìš° ì°¨íŠ¸
st.subheader("ðŸ’¡ì‚¬ìš© ë°©ë²•")
flowchart = """
digraph {
rankdir=LR;  // ê°€ë¡œ ë°©í–¥ ì„¤ì •
node [shape=box, style=filled, fillcolor="#E0E0E0", height=1.5, width=2.5]; // ë…¸ë“œ ìŠ¤íƒ€ì¼ ë° í¬ê¸° ì„¤ì •
A [label="ì‚¬ìš©ìž ìž…ë ¥" shape=ellipse fillcolor="#A3C1AD" height=1.5 width=2.5 fontsize=18]; // ê¸€ì”¨ í¬ê¸° ì„¤ì •
B [label="[ë¶„ì„í•˜ê¸°] ë²„íŠ¼ í´ë¦­" shape=ellipse fillcolor="#A3C1AD" height=1.5 width=2.5 fontsize=18];
C [label="ê²°ê³¼ í™•ì¸" shape=ellipse fillcolor="#A3C1AD" height=1.5 width=2.5 fontsize=18];

A -> B -> C;
}
"""
st.graphviz_chart(flowchart)


# ê¸°ëŠ¥ ì¶”ê°€ 
st.write("""
1. ì‚¬ìš©ìž ìž…ë ¥: ê¸€ì„ ìž…ë ¥í•©ë‹ˆë‹¤.
2. 'ë¶„ì„í•˜ê¸°'ë¥¼ ëˆ„ë¥´ë©´ ë“±ìž¥í•œ ë‹¨ì–´ë“¤ì´ ì¹´ìš´íŠ¸ ë©ë‹ˆë‹¤. 
3. ì˜ë¬¸ì˜ ê²½ìš°, ë¶ˆìš©ì–´ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. 
""")

# ì‚¬ìš©ìž ìž…ë ¥ ë°›ê¸°
user_input = st.text_area("ðŸŸ¢ ë¬¸ìž¥ì„ ìž…ë ¥í•˜ì„¸ìš”:")

# ë¶„ì„í•˜ê¸° ë²„íŠ¼
if st.button("ðŸ“¥ë¶„ì„í•˜ê¸°"):
    # ë‹¨ì–´ ì¶”ì¶œ
    words = extract_words(user_input)

    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_counts = Counter(words)

    # ìžì£¼ ë“±ìž¥í•œ ë‹¨ì–´ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    most_common_words = word_counts.most_common()

    # ê²°ê³¼ ì¶œë ¥
    if most_common_words:
        st.subheader("ðŸ—¨ï¸ë“±ìž¥í•œ ë‹¨ì–´:")
        for word, count in most_common_words:
            st.write(f"{word}: {count}íšŒ")
       
        
        # ë°” ê·¸ëž˜í”„ ì¶œë ¥
        words, counts = zip(*most_common_words)  # ë‹¨ì–´ì™€ ì¹´ìš´íŠ¸ë¥¼ ê°ê° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # Matplotlib ì‚¬ìš©
        st.markdown("#### ðŸ“Šë“±ìž¥ ë‹¨ì–´ ì‹œê°í™”")
        plt.figure(figsize=(10, 5))
        plt.barh(words, counts, color='skyblue')  # ê°€ë¡œ ë°” ê·¸ëž˜í”„
        plt.xlabel("Count")
        plt.ylabel("Word")
        plt.title("Frequency of words")
        plt.xticks(rotation=45)
        plt.gca().invert_yaxis()  # yì¶• ë°˜ì „
        st.pyplot(plt)  # Streamlitì— Matplotlib ê·¸ëž˜í”„ í‘œì‹œ
        
        st.markdown("#### ðŸŒ ë‹¨ì–´ í´ë¼ìš°ë“œ")
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)
        
        # ì›Œë“œ í´ë¼ìš°ë“œë¥¼ Matplotlibìœ¼ë¡œ ê·¸ë¦¬ê¸°
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')  # ì¶• ìˆ¨ê¸°ê¸°
        st.pyplot(plt)  # Streamlitì— ì›Œë“œ í´ë¼ìš°ë“œ í‘œì‹œ
        
           
    else:
        st.write("ìž…ë ¥ëœ ë¬¸ìž¥ì—ì„œ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


    
#################
######### ê¸°ëŠ¥ 2. 
#################
st.title("2. Sentiment Analysis - ì˜ë¬¸ version")
st.subheader("ðŸ’¡ì‚¬ìš© ë°©ë²•")

st.write("""
1. ì‚¬ìš©ìž ìž…ë ¥: reviweê´€ë ¨ csv íŒŒì¼ì„ ì²¨ë¶€í•©ë‹ˆë‹¤. 
2. ë¦¬ë·°ë‚´ìš©ì˜ ì¹¼ëŸ¼ ì´ë¦„ì€ 'contents' ìœ¼ë¡œ ë§žì¶°ì£¼ì„¸ìš”. 
3. 'ë¶„ì„í•˜ê¸°'ë¥¼ ëˆ„ë¥´ë©´ ë¦¬ë·° ê°ì„± ë¶„ì„ 
""")

# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
uploaded_file1 = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="csv", key="uploader1")


if uploaded_file1 is not None:
    # ì—…ë¡œë“œëœ CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
    df = pd.read_csv(uploaded_file1)
    
    # DataFrameì„ í™”ë©´ì— í‘œì‹œ
    st.write("ì—…ë¡œë“œëœ DataFrame:")
    st.dataframe(df)

    # 3. ê°ì„± ë¶„ì„ ìˆ˜í–‰
    if 'contents' in df.columns:
        
        def get_sentiment(text):
            analysis = TextBlob(text)
            return analysis.sentiment.polarity  # ê°ì„± ì ìˆ˜ ë°˜í™˜ (0 ~ 1 ì‚¬ì´)

        # ê°ì„± ë¶„ì„ì„ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ì¹¼ëŸ¼ ì¶”ê°€
        df['sentiment'] = df['contents'].apply(get_sentiment)

        # dataframe ì¶œë ¥
        st.write("ê°ì„±ë¶„ì„:", df[['contents', 'rating', 'sentiment']])
        
        
        positive_reviews = df[(df['sentiment'] > 0) & (df['sentiment'] <= 1)]
        negative_reviews = df[df['sentiment'] < 0]

        # p/n ì¶œë ¥
        st.write(f"ðŸ‘ê¸ì • ë¦¬ë·°: {len(positive_reviews)}ê°œ (ê¸°ì¤€:Sentiment>0)")
        st.write(f"ðŸ‘Žë¶€ì • ë¦¬ë·°: {len(negative_reviews)}ê°œ (ê¸°ì¤€:Sentiment<0)")
    else:
        st.error("DataFrameì—ëŠ” 'contents' ì¹¼ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")



#################
######### ê¸°ëŠ¥ 3. 
#################
# ëŒ€ì‹ , HTMLë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
import streamlit.components.v1 as components

st.title("3. News Topic - ì˜ë¬¸ version")
st.subheader("ðŸ’¡ì‚¬ìš© ë°©ë²•")

st.write("""
1. ì‚¬ìš©ìž ìž…ë ¥: newsê´€ë ¨ csv íŒŒì¼ì„ ì²¨ë¶€í•©ë‹ˆë‹¤. 
2. Topicê´€ë ¨ ì¹¼ëŸ¼ ì´ë¦„ì€ 'head-line'ê³¼ 'outline'ìœ¼ë¡œ ë§žì¶°ì£¼ì„¸ìš”. 
3. 'ë¶„ì„í•˜ê¸°'ë¥¼ ëˆ„ë¥´ë©´ Topicë¶„ì„ 
""")

def preprocessing(words_list):
    
    stop_words = set(stopwords.words('english'))
    # ë¶ˆìš©ì–´ë¥¼ ì œì™¸í•œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    words = [[word for word in sentence if word.lower() not in stop_words] for sentence in words_list]

    corpus_list = []
    lemmatizer = WordNetLemmatizer()

    for corpus in words:
        lemmatized_corpus = [lemmatizer.lemmatize(word, pos='n') for word in corpus]  # í‘œì œì–´ ì¶”ì¶œ
        corpus_list.append(lemmatized_corpus)
    return corpus_list    


# íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
uploaded_file2 = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="csv" , key="uploader2")


if uploaded_file2 is not None:
    
    df = pd.read_csv(uploaded_file2)
    st.write("ì—…ë¡œë“œëœ DataFrame:") # DataFrameì„ í™”ë©´ì— í‘œì‹œ
    st.dataframe(df)
    
    headlines_list = df['head-line'].tolist()
    outlines_list = df['outline'].tolist()
    combined_list = headlines_list + outlines_list

    # ë¬¸ì„œë³„ë¡œ ë‹¨ì–´ í† í°í™” (ì¤‘ì²© ë¦¬ìŠ¤íŠ¸)
    tokenized_words = [word_tokenize(sentence.lower()) for sentence in combined_list]
    
    # ì „ì²˜ë¦¬ í•¨ìˆ˜ ì ìš©  
    tokenized_words = preprocessing(tokenized_words)

    

    # gensimì˜ LDAì—ì„œ ì‚¬ìš©í•˜ëŠ” BoWì˜ í˜•íƒœëŠ” (ë‹¨ì–´ ë²ˆí˜¸: index, ë¹ˆë„)ë¡œ ì´ë¤„ì§„ ë¦¬ìŠ¤íŠ¸ ìžë£Œ
    dictionary = corpora.Dictionary(tokenized_words) 
    
    dictionary.filter_extremes(no_below=2, no_above=0.5) # ë¹ˆë„ 2ì´ìƒ í¬í•¨, ì „ì²´ 50% ì´ìƒ ë‹¨ì–´ ì œê±°
    corpus = [dictionary.doc2bow(token) for token in tokenized_words] 
    
    
    # CoherenceModelë¡œ ì ì ˆí•œ í† í”½ ìˆ˜ ì„ ì •í•˜ê¸° 
    # (ì‘ì§‘ë„ëŠ” í† í”½ì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì˜ ê´€ë ¨ì„±ì´ ì–¼ë§ˆë‚˜ ë†’ì€ì§€ë¥¼ ì¸¡ì •)
    coherence_score = [] 
    for num_topics in range(2, 6):
        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, 
                                                    num_topics=num_topics, passes=10)
        coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=tokenized_words, 
                                                           dictionary=dictionary, coherence='c_v')
        coherence_lda = coherence_model_lda.get_coherence()
          
        st.write(f'Number of Topics: {num_topics}, Coherence Score: {coherence_lda}')
        
        
        coherence_score.append(coherence_lda)

    k=[]
    for i in range(2,6):
        k.append(i)
    
    x = k
    y= coherence_score
    plt.title('Topic Coherence')
    plt.plot(x,y)
    plt.xlim(2,10)
    plt.xlabel('Number Of Topic (2-10)')
    plt.ylabel('Cohrence Score')
    # Streamlitì— ê·¸ëž˜í”„ í‘œì‹œ
    st.pyplot(plt)
    st.write(f"ðŸ‘ê°€ìž¥ ë†’ì€ ì ìˆ˜ë¥¼ ê°–ëŠ” í† í”½ ìˆ˜ê°€ ì ì ˆí•©ë‹ˆë‹¤.")

    # ì˜ˆì‹œë¡œ í† í”½ìˆ˜ 2ê°œ ì„ ì •
    final_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes= 5)
    final_model.print_topics(num_words= 4) # í† í”½ ë‹¹ ë‚˜íƒ€ë‚¼ ë‹¨ì–´ ìˆ˜
    
    # LDA ì‹œê°í™”
    prepared_data = gensimvis.prepare(final_model, corpus, dictionary)
    pyLDAvis.display(prepared_data)
    

    
    
    # HTMLë¡œ pyLDAvis ì¶œë ¥
    html = pyLDAvis.prepared_data_to_html(prepared_data)
    components.html(html, height=800)  # ë†’ì´ëŠ” í•„ìš”ì— ë”°ë¼ ì¡°ì •
    
    



